\section{Language Runtime Background}
Designers of interpreted languages peform a delicate balancing act between the inclusion of interesting language
features and the maintainence of acceptable performance. Modern production-quality interpreters are implemented
in a variety of styles that span this continuum. Simplest among these design patterns is the recursive 
``big-step''-style evaluator, which  walks the abstract syntax tree (AST) of the program in a recursive fashion
and reduces AST nodes to values. The process of adding a new feature is then as straightforward as adding a
handler for the new type of AST node. Because of its reliance on recursion and resulting tendency to exhibit
nonlinear control flow, this style of interpreter is often accompanied by a strong performance overhead.

A now-common fix to this approach is the so-called \emph{bytecode interpreter}, alluded to in the introduction.
This style of interpreter works by performing a compilation of the program AST to an intermediate, flat
bytcode language. The bytecodes are then fetched, decoded, and executed in a loop. Classically, this is done
using a switch statement that will match opcodes to their corresponding handlers.
Unfortunately, while this allows linearization of control flow, it introduces new overhead due to
bytecode fetching and branching at the hardware level (as opposed to branching in the interpreted language).

\section{Improving Branch Prediction Accuracy}
Advanced threading techniques such as direct threading and indirect threading, which improve on the bytecode
interpreter model by executing sequences of precompiled machine code to handle VM opcodes, manage to do away
with most of the instruction fetch overhead but retain the potential for overhead from branch misprediction.
This is because indirect branching is extremely frequent in interpreters, and does not follow a pattern which
can be resolved easily by today's hardware branch predictors. The branching behavior of an interpreter is
somewhat unique in that it is entirely driven by the execution flow of the program being executed, so it may
vary arbitrarily. The problem is exacerbated by the frequent tendency of dynamic languages to include features
like dynamic typing and reflection, which are themselves frequently handled by additional conditionals.

Indeed, previous research~\cite{ertl03} has found that 81\%-98\% of branches in a conventional switch-based 
bytecode interpreters are mispredicted, and even in more advanced interpreters with threaded code, the overhead
is still on the order of 57\%-63\%.
That interpreters undermine branch prediction in this manner is clearly a problem, and we discuss two attempts
at fixing the problem from researchers.

\subsection{Dynamo and DynamoRIO}
Dynamo~\cite{bala00} is a pseudo-interpreter designed to perform hot trace specialization for native code
by presenting the same interface as the processor for running native code, but transparently performing the
specialization in software at runtime. This is similar to the JIT compiler model for language VMs, but operates
at a much lower-level.

In \cite{sullivan03} Sullivan et al. give an overview of DynamoRIO, a solution for reducing emulation overhead
present in Dynamo through translation caching, developed jointly by MIT and HP Labs.
They then observe that for interpreters, DynamoRIO's trace collection heuristics do not function correctly, and
thus the impressive speedups typical of DynamoRIO are not present in this setting. This is because, as discussed,
the control flow of an interpreter is unpredictable, due to the runtime dependency on the program being executed.
Their solution was to extend DynamoRIO with an API to be used by language runtime developers. 
This API provides provides hooks to a special tracing framework to be inserted in the interpreter, which signal
the trace collector to start and stop. This means that the interpreter itself can identify basic blocks in the
interpreted code and correlate them with basic blocks in the underlying machine code that results, so that
the tracing infrastructure has much more information about the execution than it would without the hints provided
by these hooks.

\subsection{Context Threading}
In \cite{berndl05} Berndl et al. view the branch prediction accuracy problem as one of ``contexts''--the
prediction of a branch to be taken by an interpreter depends on the particular context given by the program being
executed. 
The solution that results improves specifically upon the direct-threaded interpreter architecture 
by using subroutine threading. 

In traditional direct-threaded interpreters the bytecodes are transformed into an array of labels called 
 a direct threaded table (DTT), which is then indexed by a virtual program counter (vPC).
A direct threaded instruction is a label to a specific region of the code in the interpreter that performs 
the opcode handling. After each opcode is executed the next opcode is "\emph{dispatched}" by calling 
{\tt goto DTT[vPC++];} i.e. an indirect branch instruction. 
In case of a branch instruction the vPC is computed based on the branch taken in the program being interpreted.
This negates the switch-case overhead and has good cache behavior but as discussed, will still falter due to 
branch misprediction and pipeline flushes.  

The solution to this problem is a variation on subroutine threading called \emph{context threading}. 
In context threading, the bytecode instructions of the program are transformed into an array of function call 
instructions and the interpreter executes each one of them serially. 
The control flow instructions in the program are handled the same way as in direct threaded code 
(using indirect calls), but because we are now using function calls instead of simple jumps, the interpreter
is able to provide more context for the instructions being executed, so that the branch predictor will be able
to make better decisions.
Though intuition might suggest that a series of function calls would execute slower than a series of jumps, 
results suggest otherwise. The context threading implementation described in the paper reduced the mean branch 
mispredictions by 95\% for standard benchmarks on the SableVM virtual machine and the Inria OCaml interpreter 
on the Pentium 4 microprocessor when compared to traditional direct-threaded interpreters.

\subsection{Modifying Instruction Layouts}
In \cite{casey07} Casey et al. describe two different techniques for improving branch prediction for virtual
machines at the hardware level. The techniques are instruction replication and superinstructions, and
both techniques involve restructuring the instruction stream in some way.
\subsubsecton{Instruction Replication}
We can view a program's VM instruction listing as a multiset, that is, a table of instructions together with the
number of times each instruction occurs. In the paper, this is referred to as the ``working set'' of the program.
Casey et al. observe that if an instruction occurs only once in a program's working set, then as expected, there
is no problem with branch prediction; the instruction to occur following 
