\section{Language Runtime Background}
Designers of interpreted languages peform a delicate balancing act between the inclusion of interesting language
features and the maintainence of acceptable performance. Modern production-quality interpreters are implemented
in a variety of styles that span this continuum. Simplest among these design patterns is the recursive 
``big-step''-style evaluator, which  walks the abstract syntax tree (AST) of the program in a recursive fashion
and reduces AST nodes to values. The process of adding a new feature is then as straightforward as adding a
handler for the new type of AST node. Because of its reliance on recursion and resulting tendency to exhibit
nonlinear control flow, this style of interpreter is often accompanied by a strong performance overhead.

A now-common fix to this approach is the so-called \emph{bytecode interpreter}, alluded to in the introduction.
This style of interpreter works by performing a compilation of the program AST to an intermediate, flat
bytcode language. The bytecodes are then fetched, decoded, and executed in a loop. Classically, this is done
using a switch statement that will match opcodes to their corresponding handlers.
Unfortunately, while this allows linearization of control flow, it introduces new overhead due to
bytecode fetching and branching at the hardware level (as opposed to branching in the interpreted language).

\section{Improving Branch Prediction Accuracy}
Advanced threading techniques such as direct threading and indirect threading improve on the bytecode
interpreter model by executing sequences of precompiled machine code to handle VM opcodes. They get 
rid of most of the instruction fetch overhead but retain the potential for overhead from branch misprediction.
This is because indirect branching is extremely frequent in interpreters, and does not follow a pattern which
can be resolved easily by today's hardware branch predictors. The branching behavior of an interpreter is
somewhat unique in that it is entirely driven by the execution flow of the program being executed, so it may
vary arbitrarily. The problem is exacerbated by the frequent tendency of dynamic languages to include features
like dynamic typing and reflection, which are themselves frequently handled by additional conditionals.

Indeed, previous research~\cite{ertl03} has found that 81\%-98\% of branches in a conventional switch-based 
bytecode interpreters are mispredicted, and even in more advanced interpreters with threaded code, the overhead
is still on the order of 57\%-63\%.
That interpreters undermine branch prediction in this manner is clearly a problem, and we discuss two attempts
at fixing the problem from researchers.

\subsection{Dynamo and DynamoRIO}
Dynamo~\cite{bala00} is a pseudo-interpreter designed to perform hot trace specialization for native code
by presenting the same interface as the processor for running native code, but transparently performing the
specialization in software at runtime. This is similar to the JIT compiler model for language VMs, but operates
at a much lower-level.

In \cite{sullivan03} Sullivan et al. give an overview of DynamoRIO, a solution for specializing interpreters
based on the IA-32 build of Dynamo, developed jointly by MIT and HP Labs.
They then observe that for interpreters, Dynamo's trace collection heuristics do not function correctly, and
thus the impressive speedups typical of Dynamo are not present in this setting. This is because, as discussed,
the control flow of an interpreter is unpredictable, due to the runtime dependency on the program being executed.
Their solution was to extend Dynamo with an API to be used by language runtime developers. 
This API provides provides hooks to a special tracing framework to be inserted in the interpreter, which signal
the trace collector to start and stop. This ensures that the information about the traces collected matches the
executed program flow and not merely the control flow of the interpreter.

\subsection{Context Threading}
In \cite{berndl05} Berndl et al. offer a solution for specifically improving the performance of interpreters using threaded code
by using subroutine threading. 

In traditional direct-threaded interpreters the bytecodes are transformed into an array of labels called 
 a direct threaded table (DTT), which is then indexed by a virtual program counter (vPC).
A direct threaded instruction is a label to a specific region of the code in the interpreter that performs 
the opcode handling. After each opcode is executed the next opcode is "\emph{dispatched}" by calling 
{\tt goto DTT[vPC++];} i.e. an indirect branch instruction. 
In case of a branch instruction the vPC is computed based on the branch taken in the program being interpreted.
This negates the switch-case overhead and has good cache behavior but as discussed, will still falter due to 
branch misprediction and pipeline flushes.  

The solution to this problem is a variation on subroutine threading called \emph{context threading}. 
In context threading, the bytecode instructions of the program are transformed into an array of function call 
instructions and the interpreter executes each one of them serially. 
The control flow instructions in the program are handled the same way as in direct threaded code 
(using indirect calls), but because we are now using function calls instead of simple jumps, the interpreter
is able to provide more context for the instructions being executed, so that the branch predictor will be able
to make better decisions.
Though intuition might suggest that a series of function calls would execute slower than a series of jumps, 
results suggest otherwise. The context threading implementation described in the paper reduced the mean branch 
mispredictions by 95\% for standard benchmarks on the SableVM virtual machine for Java and the Inria OCaml interpreter 
on the Pentium 4 microprocessor when compared to traditional direct-threaded interpreters.

\subsection{Modifying Instruction Layouts}
In \cite{casey07} Casey et al. describe two different techniques for improving branch prediction for virtual
machines at the hardware level. The techniques are instruction replication and superinstructions, and
both techniques involve restructuring the instruction stream in some way.
\subsubsection{Instruction Replication}
We can view a program's VM instruction listing as a multiset, that is, a table of instructions together with the
number of times each instruction occurs. In the paper, this is referred to as the ``working set'' of the program.
Casey et al. observe that if an instruction occurs only once in a program's working set, then as expected, there
is no problem with branch prediction; the instruction to occur following that instruction will be the same every
time. However, if an instruction occurs more than once in the working set, then the behavior of the
interpreter following the handling of this instruction will no longer be deterministic\footnote{This assumes the
use of a branch target buffer, which for the purposes of this paper we view in a simplistic manner.}.

The fix proposed for this is a technique called \emph{instruction replication}, which, as the name suggests,
involves making multiple copies of a given instruction. For an instruction that occurs more than once in the
working set, we can copy the instruction as many times at it occurs in the working set, and then tag each
instruction separately. In doing this, each occurrence of the instruction will have a unique entry
in the BTB, and thus for each occurrence of the instruction, the following instruction can be predicted
separately, removing the inaccuracy.

\subsubsection{Superinstructions}
The technique of using \emph{superinstructions}, which are VM instructions that represent a sequence of multiple
instructions, has been well-known outside of this context, for the purposes of code size and fetch
overhead reduction. The Casey paper, however, investigates superinstructions in the context of improving branch
prediction accuracy. The results of their experiments suggest that use of superinstructions actually reduces
mispredictions more than it reduces fetch/dispatch overhead. They note that the likely reason for this is the
fact that a superinstruction standardizes a sequence of instructions that will execute deterministically each
time the superinstruction is called.
