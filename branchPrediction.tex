\section{Branch Prediction}
Programming language design has always been a tricky area of research. More importantly the design of the runtime plays an important role in enabling interesting language features while maintaining acceptable performance. When we consider the implementation of modern language runtimes, implementing a quick interpreter has mostly been the first choice. The main reason for this is the ease of implementation and the ease of extensibility. Unfortunately, interpreters come with a large performance overhead. Classical big-step style interpreters which traverse through the AST of a program suffer from this. An improvement over this design are the bytecode based interpreters where the bytecodes are fetched, decoded and then executed in a loop. The classical way of impementing this is using switch statement matching the opcode with various possible cases and executing the corresponding opcode handler. 

The major drawbacks of this approach are the bytecode fetching and branching overhead. Other alternatives to the switch-case approach like direct threading, indirect threading also suffer from the similar branch mis-prediction overhead. There branches that are taken depend on the flow of execution or the stream of bytecodes of the program being executed. It does not follow any pattern which is very much the foundation of branch prediction strategies implemented in the hardware. It is important to note here that the branches that we are describing are the ones in the interpreter  loop and should not be confused with the branches in the program being interpreted. The problem with branch mis-prediction is even more exagerated in the case of Dynamic Scripting Languages (DSLs) where the opcode handlers for a dynamic instruction have to first determine the types of the opcodes and them perform operations based on the observed types. This is generally implemented using a \emph{if-else} chain or a \emph{switch-case} construct adding to the number of branches mis-predicted during execution.

\subsection{Dynamo and DynamoRIO}
Dynamo\cite{bala00} is an pseudo interpreter which interprets machine code and specializes the hot traces that are executed at runtime. It follows the same philosophy as a JIT compiler but at a much lower level. 

In \cite{} Sullivan et. al presented DynamoRIO, based on IA-32 version of Dynamo, as a solution to reduce the interpretation overhead. Using Dynamo or DynamoRIO naively as a binary optimizer for any interpreter does not yield any speedup. This is because DynamoRIO relies on its trace collection heuristic to optimize a binary and as mentioned above the trace of execution of any interpreter depends on the program being interpreted which is unpredictable at runtime. To solve this problem DynamioRIO infrastucture provides APIs to language runtime developers to instrument their interpreters with hooks to a special traceing framework. The hooks provide signals to the underlying framework when to start and stop the trace collection. The idea here is to make sure that the trace that is collected matches the program that is being interpreted rather than the interpreter that is interpreting it. This gives lot more information to the tracing infrastructure to work on and optimize. 

\subsection{Context Threading}
Context threading takes a different approach on solving the problem. In \cite{berndl05} Berndl et al rephrase the above problem as a "Context problem". Their solution improves upon the direct-threaded interpreter architecture by using subroutine threading. 

In direct-threaded interpreters the bytecodes are transformed into an array of labels called Direct Threaded Table (DTT) indexed by a virtual PC (vPC). A direct threaded instruction is a label to a specific region of the code in the interpreter that performs the opcode handling. After each opcode is executed the next opcode is "\emph{dispatched}" by calling {\tt goto DTT[vPC++];} i.e. indirect branch. In case of a branch instruction the vPC is computed based on the branch taken in the program being interpreted. This negates the switch-case overhead and has good cache behavior but still suffers a lot due to branch misprediction and pipeline flushes.  

The solution to this problem is a variation of subroutine threading called \emph{Context Threading}. In Context Threading the bytecode instructions of the program are transformed into an array of function call instructions and the interpreter executes each one of them serially. The control flow instructions in the program are handled the same way as direct threaded code (using indirect calls). The main idea is to provide different contexts for different instructions that are executed so that the branch predictors can make better decisions. Though at first glance executing a series of function calls seem to be slower compared to a series of jumps, the results suggest otherwise. Context threading reduced the mean branch mispredictions by 95% running standard benchmarks for SableVM(Java) and OCaml interpreter on P4 processor when compared to direct-threaded interpreter.

\subsection{Instruction replication and Superinstructions}
\cite{07casey}